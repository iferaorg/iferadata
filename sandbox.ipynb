{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ifera.ifera as ifera\n",
    "import torch\n",
    "\n",
    "importlib.reload(ifera)\n",
    "\n",
    "config = ifera.InstrumentConfig()\n",
    "instrument = config.get_config('IBKR', 'NIY:1m')\n",
    "\n",
    "# ifera.check_s3_file_exists(S3_BUCKET, ifera.make_s3_key(instrument, True))\n",
    "\n",
    "# df_raw = ifera.load_data(raw=True, instrument=instrument, zipfile=True)\n",
    "\n",
    "# df = ifera.load_data(raw=False, instrument=instrument, zipfile=True, reset=True)\n",
    "\n",
    "# instrument = config.get_config('ES@IBKR:1m')\n",
    "# t = ifera.load_data_tensor(instrument=instrument, reset=False, device=torch.device('cuda:0'), dtype=torch.float32)\n",
    "\n",
    "instrument = config.get_config('IBKR', 'NN:1m')\n",
    "t_nn = ifera.load_data_tensor(instrument=instrument, reset=False, device=torch.device('cuda:0'), dtype=torch.float32)\n",
    "\n",
    "instrument = config.get_config('IBKR', 'NIY:1m')\n",
    "t_niy = ifera.load_data_tensor(instrument=instrument, reset=False, device=torch.device('cuda:0'), dtype=torch.float32)\n",
    "\n",
    "# t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "t_nn[-60 * 8 + 30], t_niy[-60 * 8 + 30], t_nn[-60 * 8 + 34], t_niy[-60 * 8 + 34]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument = config.get_config('IBKR', 'CL:1m')\n",
    "t = ifera.load_data_tensor(instrument=instrument, reset=False, device=torch.device('cuda:0'), dtype=torch.float32)\n",
    "t[-60 * 8 + 30], dt.date.fromordinal(int(t[-60 * 8 + 30, 0].item())), dt.timedelta(seconds=t[-60 * 8 + 30, 1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ifera.load_data(raw=True, instrument=instrument, zipfile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[dt.datetime(2025, 1, 13, 9, 30)]\n",
    "import pandas as pd\n",
    "\n",
    "instrument = config.get_config('IBKR', 'NIY:1m')\n",
    "\n",
    "df = ifera.load_data(raw=True, instrument=instrument, zipfile=True)\n",
    "t = ifera.load_data_tensor(instrument=instrument, reset=False, device=torch.device('cuda:0'), dtype=torch.float32)\n",
    "\n",
    "print(df.loc[dt.datetime(2025, 1, 10, 9, 30)])\n",
    "idx = -60 * 8 + 30 - 23 * 60 \n",
    "s = dt.timedelta(seconds=t[idx, 1].item()).seconds\n",
    "f\"Date: {dt.date.fromordinal(int(t[idx, 0].item()))}, Time: {s//3600}:{(s%3600)//60}:{s%60:02d}, Open: {t[idx, 4].item():.2f}, High: {t[idx, 5].item():.2f}, Low: {t[idx, 6].item():.2f}, Close: {t[idx, 7].item():.2f}, Volume: {t[idx, 8].item():.0f}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()\n",
    "df.loc[dt.datetime(2025, 1, 10, 9, 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "response = s3.list_objects(Bucket=\"kibotdata\", Prefix='futures/1m/')\n",
    "for obj in sorted(response.get('Contents', []), key=lambda x: x['Size'], reverse=True):\n",
    "    print(obj['Key'], obj['Size'] // 1024**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ifera.ifera as ifera\n",
    "\n",
    "config = ifera.InstrumentConfig()\n",
    "broker = config.get_broker_config('IBKR')\n",
    "\n",
    "for instrument in broker.instruments:\n",
    "    instrument = config.get_config('IBKR', f\"{instrument}:1m\")\n",
    "    df = ifera.load_data(raw=False, instrument=instrument, zipfile=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ifera.ifera as ifera\n",
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "config = ifera.InstrumentConfig()\n",
    "broker = config.get_broker_config('IBKR')\n",
    "\n",
    "for key in config.instruments_data.keys():\n",
    "    base_inst = config.get_base_instrument_config(key)\n",
    "    if base_inst.symbol not in broker.instruments:\n",
    "        continue\n",
    "    instrument = config.get_config('IBKR', key)\n",
    "    t = ifera.load_data_tensor(instrument=instrument, reset=False, device=torch.device('cuda:0'), dtype=torch.float32)\n",
    "    t = rearrange(t, '(d t) c -> d t c', t = instrument.total_steps)\n",
    "    print(instrument.symbol, t.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = t[:, :, 4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ifera.ifera as ifera\n",
    "channels = {\"Open\":0, \"High\":1, \"Low\":2, \"Close\":3, \"Volume\":4}\n",
    "\n",
    "# Relative True Range: max(high, prev_close) / min(low, prev_close), and simple high/low ratio for the first bar\n",
    "rtr = torch.zeros_like(data[:, :, channels[\"Close\"]])\n",
    "rtr[:, 0] = data[:, 0, channels[\"High\"]] / data[:, 0, channels[\"Low\"]] - 1.0\n",
    "rtr[:, 1:] = torch.max(data[:, 1:, channels[\"High\"]], data[:, :-1, channels[\"Close\"]]) / torch.min(data[:, 1:, channels[\"Low\"]], data[:, :-1, channels[\"Close\"]]) - 1.0\n",
    "\n",
    "# Where volume is zero set rtr to be the same as the previous value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat\n",
    "\n",
    "channels = {\"Open\":0, \"High\":1, \"Low\":2, \"Close\":3, \"Volume\":4}\n",
    "\n",
    "volume = data[:, :, channels[\"Volume\"]].to(torch.int32)\n",
    "\n",
    "# Find first non-zero volume each day\n",
    "first_non_zero = torch.argmax((volume > 0).to(torch.int8), dim=1)\n",
    "\n",
    "rtr = torch.zeros_like(data[:, :, channels[\"Close\"]])\n",
    "rtr[:, first_non_zero] = data[:, first_non_zero, channels[\"High\"]] / data[:, first_non_zero, channels[\"Low\"]] - 1.0\n",
    "\n",
    "mask = volume != 0\n",
    "vdata = data[mask, :]\n",
    "vrtr = rtr[mask]\n",
    "\n",
    "raw_rtr = torch.max(vdata[1:, channels[\"High\"]], vdata[:-1, channels[\"Close\"]]) / torch.min(vdata[1:, channels[\"Low\"]], vdata[:-1, channels[\"Close\"]]) - 1.0\n",
    "vrtr[1:] = torch.where(vrtr[1:] == 0, raw_rtr, vrtr[1:])\n",
    "\n",
    "artr = ifera.ema_slow(vrtr, 1/14)\n",
    "\n",
    "artr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "def ema(x: torch.Tensor, alpha: float, chunk_size: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Calculate the exponential moving average of a tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.Tensor\n",
    "        Input tensor. The last dimension is the one to calculate the moving average over.\n",
    "    alpha : float\n",
    "        Smoothing factor between 0 and 1.\n",
    "    chunk_size : Optional[int], default=None\n",
    "        If provided, process the series in chunks of this size to reduce memory usage.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y : torch.Tensor\n",
    "        Output tensor with the same shape as x, containing the EMA along the last dimension.\n",
    "    \"\"\"\n",
    "    # Get tensor properties\n",
    "    shape = x.shape\n",
    "    n = shape[-1]\n",
    "    device = x.device\n",
    "    dtype = x.dtype\n",
    "\n",
    "    if chunk_size is None or chunk_size >= n:\n",
    "        # Use original method for small series or if chunk_size not specified\n",
    "        i = torch.arange(n, device=device).unsqueeze(0)  # Shape: (1, n)\n",
    "        j = torch.arange(n, device=device).unsqueeze(1)  # Shape: (n, 1)\n",
    "        diff = i - j  # Shape: (n, n)\n",
    "        W = torch.where(diff >= 0, (1 - alpha) ** diff, torch.zeros_like(diff, dtype=dtype))\n",
    "        W[1:, :] *= alpha  # First row unchanged (W[0, 0] = 1), others scaled\n",
    "        return x @ W\n",
    "    else:\n",
    "        # Process in chunks to save memory\n",
    "        chunks = torch.split(x, chunk_size, dim=-1)  # Split along last dimension\n",
    "        y_chunks = []\n",
    "        y_prev = None\n",
    "\n",
    "        for idx, x_chunk in enumerate(chunks):\n",
    "            m = x_chunk.shape[-1]  # Size of current chunk\n",
    "            i = torch.arange(m, device=device).unsqueeze(0)\n",
    "            j = torch.arange(m, device=device).unsqueeze(1)\n",
    "            diff = i - j\n",
    "            W_local = torch.where(diff >= 0, (1 - alpha) ** diff, \n",
    "                                 torch.zeros_like(diff, dtype=dtype))\n",
    "\n",
    "            if idx == 0:\n",
    "                # First chunk: mimic original EMA starting condition\n",
    "                W_local[1:, :] *= alpha  # W_local[0, 0] = 1, rest scaled\n",
    "            else:\n",
    "                # Subsequent chunks: compute local EMA, all rows scaled\n",
    "                W_local *= alpha\n",
    "\n",
    "            y_local = x_chunk @ W_local\n",
    "\n",
    "            if idx > 0:\n",
    "                # Add decayed contribution from previous chunk\n",
    "                decay = (1 - alpha) ** torch.arange(1, m + 1).to(device)  # Shape: (m,)\n",
    "                y_chunk = y_local + decay * y_prev[..., None]  # Broadcasting: (..., m)\n",
    "            else:\n",
    "                y_chunk = y_local\n",
    "\n",
    "            y_chunks.append(y_chunk)\n",
    "            y_prev = y_chunk[..., -1]  # Shape: (...), last EMA value for next chunk\n",
    "\n",
    "        return torch.cat(y_chunks, dim=-1)  # Concatenate along last dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artr2 =  ema(vrtr, 1/14, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artr_final = torch.zeros_like(rtr)\n",
    "artr_final[mask] = artr2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artr_final[0,0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]], dtype=torch.float32)\n",
    "window = 3\n",
    "\n",
    "n = min(t.shape[-1], window)\n",
    "tail = t.unfold(dimension=-1, size=n, step=1).mean(dim=-1)\n",
    "\n",
    "head_nom = torch.cumsum(t[..., :n-1], dim=-1)\n",
    "head_denom = torch.arange(1, n, device=t.device, dtype=t.dtype)\n",
    "head = head_nom / head_denom\n",
    "\n",
    "sma = torch.cat([head, tail], dim=-1)\n",
    "sma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ifera.series as s\n",
    "import ifera.masked_series as ms\n",
    "from torch.masked import MaskedTensor, masked_tensor\n",
    "from einops import repeat, rearrange\n",
    "\n",
    "t = torch.tensor([float('nan'), 1.0, 2.0, float('nan'), float('nan'), 3.0, 4.0, 5.0, float('nan'), float('nan')], dtype=torch.float32)\n",
    "t = repeat(t, 't -> t c', c = 5).clone()\n",
    "t[:, 1] = t[:, 0] + 1.0 + (0.4 * torch.randn_like(t[:, 0])).round(decimals=2)\n",
    "t[:, 2] = t[:, 0] - 1.0 + (0.4 * torch.randn_like(t[:, 0])).round(decimals=2)\n",
    "t[:, 3] = t[:, 0] + 0.0 + (0.4 * torch.randn_like(t[:, 0])).round(decimals=2)\n",
    "t[:, 4] = t[:,0] * 1000\n",
    "t[:, 0:4] += 10.0\n",
    "mask = ~torch.isnan(t)\n",
    "t = repeat(t, 't c -> d t c', d = 2)\n",
    "mask = repeat(mask, 't c -> d t c', d = 2)\n",
    "\n",
    "mt = masked_tensor(t, mask)\n",
    "\n",
    "#ms.masked_rtr(mt), t[1, 1]/t[1,2] -1, torch.max(t[5, 1], t[2, 3]) / torch.min(t[5, 2], t[2, 3]) - 1.0\n",
    "\n",
    "# t_cn = rearrange(t, 'd t c -> d c t')\n",
    "# mask_cn = rearrange(mask, 'd t c -> d c t')\n",
    "# ct = ms.compress_tensor(t_cn, mask_cn)\n",
    "# ct = rearrange(ct, 'd c t -> d t c').nan_to_num(nan=1.0)\n",
    "\n",
    "ms.masked_rtr(mt), ms.masked_artr(mt, 0.2, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
