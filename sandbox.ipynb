{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ifera\n",
    "import torch\n",
    "\n",
    "importlib.reload(ifera)\n",
    "\n",
    "cm = ifera.ConfigManager()\n",
    "# instrument = config.get_config('IBKR', 'NIY:1m')\n",
    "\n",
    "# ifera.check_s3_file_exists(S3_BUCKET, ifera.make_s3_key(instrument, True))\n",
    "\n",
    "# df_raw = ifera.load_data(raw=True, instrument=instrument, zipfile=True)\n",
    "\n",
    "# df = ifera.load_data(raw=False,  instrument=instrument, zipfile=True, reset=True)\n",
    "\n",
    "# instrument = config.get_config('IBKR', 'HE:1m')\n",
    "# t_old = ifera.load_data_tensor(instrument=instrument, reset=False, device=torch.device('cuda:0'), dtype=torch.float32)\n",
    "# t_new = ifera.load_data_tensor(instrument=instrument, reset=True, device=torch.device('cuda:0'), dtype=torch.float32)\n",
    "\n",
    "# t_new.isclose(t_old).all()\n",
    "\n",
    "# instrument = config.get_config('IBKR', 'NN:1m')\n",
    "# t_nn = ifera.load_data_tensor(instrument=instrument, reset=False, device=torch.device('cuda:0'), dtype=torch.float32)\n",
    "\n",
    "iconfig = cm.get_config('IBKR', 'GF:1m')\n",
    "#t = ifera.load_data_tensor(instrument=instrument, reset=False, device=torch.device('cuda:0'), dtype=torch.float32)\n",
    "\n",
    "instrument2 = cm.get_config('IBKR', 'GF:1m')\n",
    "instrument3 = iconfig.model_copy()\n",
    "\n",
    "iconfig.__eq__(instrument2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "all_steps = pd.timedelta_range(start=pd.Timedelta(0), end=pd.Timedelta(days=1), freq=iconfig.time_step)\n",
    "# Filter out steps that are greater than or equal to instrument end time\n",
    "all_steps[all_steps < iconfig.trading_end - iconfig.trading_start][-1]\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ifera\n",
    "\n",
    "cm = ifera.ConfigManager()\n",
    "brokerConfig = cm.get_broker_config('IBKR')\n",
    "\n",
    "for key, baseConfig in cm.instruments_data.items():\n",
    "    if baseConfig['symbol'] in brokerConfig.instruments.keys() and baseConfig['type'] == 'futures':\n",
    "        iconfig = cm.get_config('IBKR', key)\n",
    "        t = ifera.load_data_tensor(instrument=iconfig, reset=False, device=torch.device('cuda:0'), dtype=torch.float32)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata2 = ifera.InstrumentData(iconfig, device=torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = ifera.MarketSimulatorIntraday(idata2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ifera\n",
    "device=torch.device('cuda:0')\n",
    "\n",
    "cm = ifera.ConfigManager()\n",
    "iconfig = cm.get_config('IBKR', 'CL:1m')\n",
    "dm = ifera.DataManager()\n",
    "idata2 = dm.get_instrument_data(iconfig, device=device)\n",
    "ms = ifera.MarketSimulatorIntraday(idata2)\n",
    "t = (iconfig.liquid_start - iconfig.trading_start).total_seconds() // iconfig.time_step.total_seconds()\n",
    "\n",
    "date_idx = torch.arange(0, idata2.data.shape[1], dtype=torch.int32, device=device)\n",
    "time_idx = torch.full_like(date_idx, t, dtype=torch.int32, device=device)\n",
    "position = torch.zeros_like(date_idx, dtype=torch.int32, device=device)\n",
    "action = torch.full_like(date_idx, 1, dtype=torch.int32, device=device)\n",
    "stop_loss = torch.full_like(date_idx, torch.nan, dtype=torch.float32, device=device)\n",
    "\n",
    "%timeit ms.calculate_step(date_idx, time_idx, position, action, stop_loss)\n",
    "#idata.data[0, 0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat\n",
    "\n",
    "parent_data = idata2.data\n",
    "multiplier = 120\n",
    "parent_steps = parent_data.shape[1]\n",
    "padding = parent_steps - parent_steps // multiplier * multiplier\n",
    "padding_data = torch.zeros((parent_data.shape[0], parent_data.shape[2]), dtype=parent_data.dtype, device=parent_data.device)\n",
    "padding_data[:, 0:4] = parent_data[:, -1, 3, None]  # Open, High, Low, Close <- Last Close\n",
    "padding_data = repeat(padding_data, \"d c -> d g c\", g=padding)\n",
    "parent_data = torch.cat([parent_data, padding_data], dim=1)\n",
    "rearrange(parent_data, \"d (g n) c -> d g c n\", n=120).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata2.instrument.contract_multiplier * (1051.0 - idata2.instrument.min_slippage - 1051.0) - 2.25, idata2.instrument.min_slippage * idata2.instrument.contract_multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "iconfig = cm.get_config('IBKR', 'CL:1m')\n",
    "t = ifera.load_data_tensor(instrument=iconfig, reset=False, device=torch.device('cuda:0'), dtype=torch.float32)\n",
    "t[-60 * 8 + 30], dt.date.fromordinal(int(t[-60 * 8 + 30, 0].item())), dt.timedelta(seconds=t[-60 * 8 + 30, 1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ifera.load_data(raw=True, instrument=iconfig, zipfile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[dt.datetime(2025, 1, 13, 9, 30)]\n",
    "import pandas as pd\n",
    "\n",
    "iconfig = cm.get_config('IBKR', 'NIY:1m')\n",
    "\n",
    "df = ifera.load_data(raw=True, instrument=iconfig, zipfile=True)\n",
    "t = ifera.load_data_tensor(instrument=iconfig, reset=False, device=torch.device('cuda:0'), dtype=torch.float32)\n",
    "\n",
    "print(df.loc[dt.datetime(2025, 1, 10, 9, 30)])\n",
    "pos = -60 * 8 + 30 - 23 * 60 \n",
    "s = dt.timedelta(seconds=t[pos, 1].item()).seconds\n",
    "f\"Date: {dt.date.fromordinal(int(t[pos, 0].item()))}, Time: {s//3600}:{(s%3600)//60}:{s%60:02d}, Open: {t[pos, 4].item():.2f}, High: {t[pos, 5].item():.2f}, Low: {t[pos, 6].item():.2f}, Close: {t[pos, 7].item():.2f}, Volume: {t[pos, 8].item():.0f}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()\n",
    "df.loc[dt.datetime(2025, 1, 10, 9, 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "response = s3.list_objects(Bucket=\"kibotdata\", Prefix='futures/1m/')\n",
    "for obj in sorted(response.get('Contents', []), key=lambda x: x['Size'], reverse=True):\n",
    "    print(obj['Key'], obj['Size'] // 1024**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ifera\n",
    "\n",
    "cm = ifera.ConfigManager()\n",
    "broker = cm.get_broker_config('IBKR')\n",
    "\n",
    "for iconfig in broker.instruments:\n",
    "    iconfig = cm.get_config('IBKR', f\"{iconfig}:1m\")\n",
    "    df = ifera.load_data(raw=False, instrument=iconfig, zipfile=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ifera\n",
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "cm = ifera.ConfigManager()\n",
    "broker = cm.get_broker_config('IBKR')\n",
    "\n",
    "for key in cm.instruments_data.keys():\n",
    "    base_inst = cm.get_base_instrument_config(key)\n",
    "    if base_inst.symbol not in broker.instruments:\n",
    "        continue\n",
    "    iconfig = cm.get_config('IBKR', key)\n",
    "    t = ifera.load_data_tensor(instrument=iconfig, reset=False, device=torch.device('cuda:0'), dtype=torch.float32)\n",
    "    t = rearrange(t, '(d t) c -> d t c', t = iconfig.total_steps)\n",
    "    print(iconfig.symbol, t.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = t[:, :, 4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ifera\n",
    "channels = {\"Open\":0, \"High\":1, \"Low\":2, \"Close\":3, \"Volume\":4}\n",
    "\n",
    "# Relative True Range: max(high, prev_close) / min(low, prev_close), and simple high/low ratio for the first bar\n",
    "rtr = torch.zeros_like(data[:, :, channels[\"Close\"]])\n",
    "rtr[:, 0] = data[:, 0, channels[\"High\"]] / data[:, 0, channels[\"Low\"]] - 1.0\n",
    "rtr[:, 1:] = torch.max(data[:, 1:, channels[\"High\"]], data[:, :-1, channels[\"Close\"]]) / torch.min(data[:, 1:, channels[\"Low\"]], data[:, :-1, channels[\"Close\"]]) - 1.0\n",
    "\n",
    "# Where volume is zero set rtr to be the same as the previous value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat\n",
    "\n",
    "channels = {\"Open\":0, \"High\":1, \"Low\":2, \"Close\":3, \"Volume\":4}\n",
    "\n",
    "volume = data[:, :, channels[\"Volume\"]].to(torch.int32)\n",
    "\n",
    "# Find first non-zero volume each day\n",
    "first_non_zero = torch.argmax((volume > 0).to(torch.int8), dim=1)\n",
    "\n",
    "rtr = torch.zeros_like(data[:, :, channels[\"Close\"]])\n",
    "rtr[:, first_non_zero] = data[:, first_non_zero, channels[\"High\"]] / data[:, first_non_zero, channels[\"Low\"]] - 1.0\n",
    "\n",
    "mask = volume != 0\n",
    "vdata = data[mask, :]\n",
    "vrtr = rtr[mask]\n",
    "\n",
    "raw_rtr = torch.max(vdata[1:, channels[\"High\"]], vdata[:-1, channels[\"Close\"]]) / torch.min(vdata[1:, channels[\"Low\"]], vdata[:-1, channels[\"Close\"]]) - 1.0\n",
    "vrtr[1:] = torch.where(vrtr[1:] == 0, raw_rtr, vrtr[1:])\n",
    "\n",
    "artr = ifera.ema_slow(vrtr, 1/14)\n",
    "\n",
    "artr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]], dtype=torch.float32)\n",
    "window = 3\n",
    "\n",
    "n = min(t.shape[-1], window)\n",
    "tail = t.unfold(dimension=-1, size=n, step=1).mean(dim=-1)\n",
    "\n",
    "head_nom = torch.cumsum(t[..., :n-1], dim=-1)\n",
    "head_denom = torch.arange(1, n, device=t.device, dtype=t.dtype)\n",
    "head = head_nom / head_denom\n",
    "\n",
    "sma = torch.cat([head, tail], dim=-1)\n",
    "sma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ifera\n",
    "from einops import rearrange\n",
    "\n",
    "cm = ifera.ConfigManager()\n",
    "iconfig = cm.get_config(\"IBKR\", \"CL:1m\")\n",
    "\n",
    "dm = ifera.DataManager()\n",
    "idata = dm.get_instrument_data(iconfig)\n",
    "\n",
    "batch_size = idata.data.shape[0]-250\n",
    "\n",
    "openPolicy = ifera.AlwaysOpenPolicy(direction=1)\n",
    "initStopPolicy = ifera.InitialArtrStopLossPolicy(\n",
    "    instrument_data=idata, atr_multiple=3.0\n",
    ")\n",
    "maintenancePolicy = ifera.ScaledArtrMaintenancePolicy(\n",
    "    instrument_data=idata,\n",
    "    stages=[\"1m\", \"5m\", \"15m\", \"1h\", \"4h\", \"1d\"],\n",
    "    atr_multiple=3.0,\n",
    "    wait_for_breakeven=False,\n",
    "    minimum_improvement=0.0,\n",
    ")\n",
    "\n",
    "tradingPolicy = ifera.TradingPolicy(\n",
    "    instrument_data=idata,\n",
    "    open_position_policy=openPolicy,\n",
    "    initial_stop_loss_policy=initStopPolicy,\n",
    "    position_maintenance_policy=maintenancePolicy,\n",
    ")\n",
    "\n",
    "#tradingPolicy = torch.compile(tradingPolicy)\n",
    "\n",
    "ms = ifera.MarketSimulatorIntraday(instrument_data=idata)\n",
    "\n",
    "t = (iconfig.liquid_start - iconfig.trading_start).total_seconds() // iconfig.time_step.total_seconds() - 1\n",
    "\n",
    "date_idx = torch.arange(0, batch_size, dtype=torch.int32, device=idata.device)\n",
    "time_idx = torch.full_like(date_idx, t, dtype=torch.int32, device=idata.device)\n",
    "position = torch.zeros_like(date_idx, dtype=torch.int32, device=idata.device)\n",
    "prev_stop_loss = torch.full_like(date_idx, torch.nan, dtype=torch.float32, device=idata.device)\n",
    "execution_price = torch.full_like(date_idx, torch.nan, dtype=torch.float32, device=idata.device)\n",
    "\n",
    "total_profit = torch.zeros_like(date_idx, dtype=torch.float32, device=idata.device)\n",
    "\n",
    "# Open position\n",
    "action, stop_loss = tradingPolicy(date_idx, time_idx, position, prev_stop_loss, execution_price)\n",
    "time_idx += 1\n",
    "profit, position, execution_price, _ = ms.calculate_step(date_idx, time_idx, position, action, stop_loss)\n",
    "position = position.clone()\n",
    "total_profit += profit\n",
    "\n",
    "while (position != 0).any():\n",
    "    prev_stop_loss = stop_loss.clone()\n",
    "    action, stop_loss = tradingPolicy(date_idx, time_idx, position, prev_stop_loss, execution_price)\n",
    "    date_idx, time_idx = idata.get_next_indices(date_idx, time_idx)\n",
    "    action = torch.where((date_idx == idata.data.shape[0] - 1) & (time_idx == idata.data.shape[1] - 1), -position, 0)\n",
    "    profit, position, execution_price, _ = ms.calculate_step(date_idx, time_idx, position, action, stop_loss)\n",
    "    position = position.clone()\n",
    "    total_profit += profit\n",
    "\n",
    "max_idx = total_profit.argmax()\n",
    "print(f\"Max index: {max_idx}, Total profit: {total_profit[max_idx].item():.4f}, date_idx: {date_idx[max_idx].item()}, time_idx: {time_idx[max_idx].item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "t = torch.arange(0, 10, dtype=torch.int32)\n",
    "mask = torch.zeros_like(t, dtype=torch.bool)\n",
    "\n",
    "t, mask, torch.cat((t[mask], torch.tensor([np.iinfo(np.int32).max], dtype=torch.int32)), dim=0).min().item(), t[t< 5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ifera\n",
    "import torch\n",
    "\n",
    "instrument = ifera.ConfigManager().get_base_instrument_config(\"TN:1m\")\n",
    "url = ifera.make_instrument_url(ifera.Scheme.FILE, ifera.Source.PROCESSED, instrument)\n",
    "# print(url)\n",
    "\n",
    "#t = ifera.load_data_tensor(instrument=instrument, reset=False, device=torch.device('cuda:0'), dtype=torch.float32)\n",
    "fm = ifera.FileManager()\n",
    "fm.refresh_file(\"adasd\")\n",
    "\n",
    "# t.shape\n",
    "# print(ifera.FileOperations.exists(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ifera\n",
    "import torch\n",
    "\n",
    "dm = ifera.DataManager()\n",
    "cm = ifera.ConfigManager()\n",
    "#parent_instrument = cm.get_base_instrument_config(\"TN\", \"1m\")\n",
    "#instrument = cm.create_derived_base_config(parent_instrument, \"5m\")\n",
    "instrument = cm.get_base_instrument_config(\"TN\", \"30m\")\n",
    "device=torch.device(\"cuda:0\")\n",
    "\n",
    "idata = dm.get_instrument_data(instrument_config=instrument, dtype=torch.float32, device=device)\n",
    "\n",
    "idata.data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ifera\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "dm = ifera.DataManager()\n",
    "cm = ifera.ConfigManager()\n",
    "instrument = cm.get_base_instrument_config(\"TN:1m\")\n",
    "idata = dm.get_instrument_data(instrument_config=instrument, dtype=torch.float32, device=torch.device(\"cuda:0\"))\n",
    "print(idata.data.shape)\n",
    "ms = ifera.MarketSimulatorIntraday(instrument_data=idata, broker_name=\"IBKR\")\n",
    "\n",
    "date_idx = torch.zeros(idata.data.shape[0], dtype=torch.int32, device=idata.device)\n",
    "time_idx = torch.full_like(date_idx, 0, dtype=torch.int32, device=idata.device)\n",
    "position = torch.full_like(date_idx, 1, dtype=torch.int32, device=idata.device)\n",
    "action = torch.full_like(date_idx, 0, dtype=torch.int32, device=idata.device)\n",
    "stop_loss = torch.full_like(date_idx, torch.nan, dtype=torch.float32, device=idata.device)\n",
    "\n",
    "def run_market_simulation(ms, idata):\n",
    "    date_idx = torch.randint(0, idata.data.shape[0] - total_days - 2, (idata.data.shape[0],), dtype=torch.int32, device=idata.device)\n",
    "    time_idx = torch.randint_like(date_idx, 0, idata.data.shape[1] - 1, dtype=torch.int32, device=idata.device)\n",
    "    position = torch.full_like(date_idx, 0, dtype=torch.int32, device=idata.device)\n",
    "    action = torch.full_like(date_idx, 1, dtype=torch.int32, device=idata.device)\n",
    "    stop_loss = torch.full_like(date_idx, torch.nan, dtype=torch.float32, device=idata.device)\n",
    "    total_profit = torch.zeros_like(date_idx, dtype=torch.float32, device=idata.device)\n",
    "    steps = torch.tensor(0, dtype=torch.int32, device=idata.device)\n",
    "    max_days = 4\n",
    "\n",
    "    while date_idx[0] < max_days or time_idx[0] < idata.data.shape[1] - 1:\n",
    "        profit, new_position, _, _ = ms.calculate_step(date_idx, time_idx, position, action, stop_loss)\n",
    "        position = new_position.clone()\n",
    "        action = torch.zeros_like(date_idx, dtype=torch.int32, device=idata.device)\n",
    "        date_idx, time_idx = idata.get_next_indices(date_idx, time_idx)\n",
    "        total_profit += profit\n",
    "        steps += 1\n",
    "    \n",
    "    return total_profit, steps\n",
    "\n",
    "def run_market_simulation_tqdm(ms, idata):\n",
    "    total_days = 5\n",
    "    date_idx = torch.randint(0, idata.data.shape[0] - total_days - 2, (idata.data.shape[0],), dtype=torch.int32, device=idata.device)\n",
    "    time_idx = torch.randint_like(date_idx, 0, idata.data.shape[1] - 1, dtype=torch.int32, device=idata.device)\n",
    "    position = torch.full_like(date_idx, 0, dtype=torch.int32, device=idata.device)\n",
    "    action = torch.full_like(date_idx, 1, dtype=torch.int32, device=idata.device)\n",
    "    stop_loss = torch.full_like(date_idx, torch.nan, dtype=torch.float32, device=idata.device)\n",
    "    total_profit = torch.zeros_like(date_idx, dtype=torch.float32, device=idata.device)\n",
    "    steps = torch.tensor(0, dtype=torch.int32, device=idata.device)\n",
    "    total_steps = total_days * idata.data.shape[1]\n",
    "    pbar = tqdm(total=total_steps, desc=\"Market Simulation\", unit=\"step\", leave=False)\n",
    "\n",
    "    while steps < total_steps:\n",
    "        profit, new_position, _, _ = ms.calculate_step(date_idx, time_idx, position, action, stop_loss)\n",
    "        total_profit += profit\n",
    "        position = new_position.clone()\n",
    "        action = torch.where(position == 0, action, 0)\n",
    "        date_idx, time_idx = idata.get_next_indices(date_idx, time_idx)\n",
    "        steps += 1\n",
    "        pbar.update(1)\n",
    "    \n",
    "    return total_profit, steps\n",
    "\n",
    "\n",
    "run_market_simulation_compiled = torch.compile(run_market_simulation)\n",
    "run_market_simulation_tqdm_compiled = torch.compile(run_market_simulation_tqdm)\n",
    "\n",
    "# Warmup calculate_step\n",
    "profit, _, _, _ = ms.calculate_step(date_idx, time_idx, position, action, stop_loss)\n",
    "profit, _, _, _ = ms.calculate_step(date_idx, time_idx, position, action, stop_loss)\n",
    "profit, _, _, _ = ms.calculate_step(date_idx, time_idx, position, action, stop_loss)\n",
    "\n",
    "# Run the market simulation\n",
    "start_t = time.time()\n",
    "total_profit, steps = run_market_simulation(ms, idata)\n",
    "print(f\"Time taken: {time.time() - start_t:.2f} seconds\")\n",
    "\n",
    "# Warmup the compiled function\n",
    "total_profit, steps = run_market_simulation_compiled(ms, idata)\n",
    "total_profit, steps = run_market_simulation_compiled(ms, idata)\n",
    "start_t = time.time()\n",
    "total_profit, steps = run_market_simulation_compiled(ms, idata)\n",
    "print(f\"Time taken: {time.time() - start_t:.2f} seconds\")\n",
    "\n",
    "# Run uncompiled function with tqdm\n",
    "start_t = time.time()\n",
    "total_profit, steps = run_market_simulation_tqdm(ms, idata)\n",
    "print(f\"Time taken: {time.time() - start_t:.2f} seconds\")\n",
    "\n",
    "# Warmup the compiled function with tqdm\n",
    "total_profit, steps = run_market_simulation_tqdm_compiled(ms, idata)\n",
    "total_profit, steps = run_market_simulation_tqdm_compiled(ms, idata)\n",
    "start_t = time.time()\n",
    "total_profit, steps = run_market_simulation_tqdm_compiled(ms, idata)\n",
    "print(f\"Time taken: {time.time() - start_t:.2f} seconds\")\n",
    "print(total_profit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore.exceptions\n",
    "\n",
    "client = boto3.client(\"s3\")\n",
    "bucket = \"ifera-marketdata\"\n",
    "\n",
    "try:\n",
    "    obj = client.head_object(Bucket=bucket, Key='processed/futures_individual/30m/YM-Z21.zip')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    print(e.response['Error'])\n",
    "\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "timedelta(hours=16,minutes=24,seconds=47) / timedelta(minutes=30,seconds=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch._higher_order_ops.foreach_map import foreach_map\n",
    "\n",
    "class TreeNode:\n",
    "    \"\"\"\n",
    "    A node in the decision tree.\n",
    "    \"\"\"\n",
    "    __slots__ = (\"is_leaf\", \"prediction\", \"feature_index\", \"threshold\", \"left\", \"right\", \"depth\")\n",
    "\n",
    "    def __init__(self, is_leaf=False, prediction=None, feature_index=None, threshold=None, left=None, right=None, depth=0):\n",
    "        self.is_leaf = is_leaf\n",
    "        self.prediction = prediction\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.depth = depth\n",
    "\n",
    "\n",
    "class DecisionTreeRegressorTorch:\n",
    "    \"\"\"\n",
    "    A regression decision tree implemented in PyTorch with iterative breadth-first build.\n",
    "\n",
    "    Splits nodes to minimize mean squared error (MSE).\n",
    "    Supports max_depth and a minimum impurity decrease threshold.\n",
    "    Uses PyTorch 2.7 torch._foreach_map for parallel split search at each depth.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth: int = 5, min_impurity_decrease: float = 1e-7):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.root = None\n",
    "\n",
    "    #@torch.compile\n",
    "    def fit(self, X: torch.Tensor, y: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Build the tree by iterating over depth levels.\n",
    "        \"\"\"\n",
    "        assert X.dim() == 2 and y.dim() == 1 and X.size(0) == y.size(0)\n",
    "\n",
    "        # Initialize root node\n",
    "        self.root = TreeNode()\n",
    "        # Track nodes, inputs, targets across depths\n",
    "        nodes = [self.root]\n",
    "        inputs = [X]\n",
    "        targets = [y]\n",
    "\n",
    "        for depth in range(self.max_depth):\n",
    "            next_nodes = []\n",
    "            next_inputs = []\n",
    "            next_targets = []\n",
    "\n",
    "            # Compute parent variances in parallel using foreach_map\n",
    "            \n",
    "            parent_vars = foreach_map(torch.var, targets, correction=0)\n",
    "            # Find best splits for all nodes at this depth in parallel\n",
    "            splits = foreach_map(self._find_best_split, inputs, targets, parent_vars)\n",
    "\n",
    "            for node, X_sub, y_sub, split in zip(nodes, inputs, targets, splits):\n",
    "                feature, threshold, imp_dec = split\n",
    "                n_samples = y_sub.numel()\n",
    "                node.depth = depth\n",
    "                # Stop splitting if no valid split, too few samples, or insufficient decrease\n",
    "                if feature is None or n_samples < 2 or imp_dec < self.min_impurity_decrease:\n",
    "                    node.is_leaf = True\n",
    "                    node.prediction = y_sub.mean().item()\n",
    "                else:\n",
    "                    node.is_leaf = False\n",
    "                    node.feature_index = feature\n",
    "                    node.threshold = threshold\n",
    "\n",
    "                    # Partition data\n",
    "                    mask = X_sub[:, feature] <= threshold\n",
    "                    X_left, y_left = X_sub[mask], y_sub[mask]\n",
    "                    X_right, y_right = X_sub[~mask], y_sub[~mask]\n",
    "\n",
    "                    node.left = TreeNode()\n",
    "                    node.right = TreeNode()\n",
    "\n",
    "                    # Queue children for next level\n",
    "                    next_nodes.extend([node.left, node.right])\n",
    "                    next_inputs.extend([X_left, X_right])\n",
    "                    next_targets.extend([y_left, y_right])\n",
    "\n",
    "            # Move to next depth\n",
    "            nodes, inputs, targets = next_nodes, next_inputs, next_targets\n",
    "            if len(nodes) == 0:\n",
    "                break\n",
    "\n",
    "        # Any remaining nodes become leaves\n",
    "        for node, y_sub in zip(nodes, targets):\n",
    "            node.is_leaf = True\n",
    "            node.prediction = y_sub.mean().item()\n",
    "\n",
    "    def predict(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict target values for input X.\n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        for x in X:\n",
    "            node = self.root\n",
    "            while not node.is_leaf:\n",
    "                node = node.left if x[node.feature_index] <= node.threshold else node.right\n",
    "            preds.append(node.prediction)\n",
    "        return torch.tensor(preds, dtype=X.dtype)\n",
    "\n",
    "    def _find_best_split(self, X: torch.Tensor, y: torch.Tensor, parent_variance: float):  # noqa: C901\n",
    "        \"\"\"\n",
    "        Vectorized single-node split search used by foreach_map.\n",
    "        Returns (feature_index, threshold, impurity_decrease).\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        if n_samples < 2:\n",
    "            return None, None, 0.0\n",
    "\n",
    "        # Sort features and align targets\n",
    "        sorted_vals, sorted_idx = torch.sort(X, dim=0)\n",
    "        y_sorted = y[sorted_idx]\n",
    "\n",
    "        # Cumulative sums\n",
    "        csum_y = torch.cumsum(y_sorted, dim=0)\n",
    "        csum_y2 = torch.cumsum(y_sorted * y_sorted, dim=0)\n",
    "\n",
    "        # Split counts\n",
    "        left_counts = torch.arange(1, n_samples, device=X.device).unsqueeze(1).expand(n_samples-1, n_features)\n",
    "        right_counts = n_samples - left_counts\n",
    "\n",
    "        # Sums and sums of squares\n",
    "        left_sum = csum_y[:-1]\n",
    "        left_sum2 = csum_y2[:-1]\n",
    "        y_tot = y.sum()\n",
    "        y2_tot = (y * y).sum()\n",
    "        right_sum = y_tot - left_sum\n",
    "        right_sum2 = y2_tot - left_sum2\n",
    "\n",
    "        # Compute variances\n",
    "        left_var = left_sum2 / left_counts - (left_sum / left_counts) ** 2\n",
    "        right_var = right_sum2 / right_counts - (right_sum / right_counts) ** 2\n",
    "\n",
    "        # Weighted child variance\n",
    "        child_var = (left_counts * left_var + right_counts * right_var) / n_samples\n",
    "\n",
    "        # Impurity decrease\n",
    "        imp_dec = parent_variance - child_var\n",
    "        distinct = sorted_vals[:-1] != sorted_vals[1:]\n",
    "        imp_dec = imp_dec.masked_fill(~distinct, float('-inf'))\n",
    "\n",
    "        # Select best split\n",
    "        max_val, idx = imp_dec.reshape(-1).max(dim=0)\n",
    "        if max_val <= 0:\n",
    "            return None, None, 0.0\n",
    "\n",
    "        feat = idx % n_features\n",
    "        pos = idx // n_features\n",
    "        thresh = (sorted_vals[pos, feat] + sorted_vals[pos+1, feat]) / 2.0\n",
    "        return feat.item(), thresh.item(), max_val.item()\n",
    "\n",
    "    def get_leaf_nodes(self) -> list:\n",
    "        \"\"\"\n",
    "        Return all leaf nodes in the tree.\n",
    "        \"\"\"\n",
    "        leaves = []\n",
    "        stack = [self.root]\n",
    "        while stack:\n",
    "            node = stack.pop()\n",
    "            if node.is_leaf:\n",
    "                leaves.append(node)\n",
    "            else:\n",
    "                # push children to stack\n",
    "                if node.right is not None:\n",
    "                    stack.append(node.right)\n",
    "                if node.left is not None:\n",
    "                    stack.append(node.left)\n",
    "        return leaves\n",
    "\n",
    "    def get_leaves_sorted(self) -> list:\n",
    "        \"\"\"\n",
    "        Return leaf nodes sorted by their prediction values (ascending).\n",
    "        \"\"\"\n",
    "        leaves = self.get_leaf_nodes()\n",
    "        return sorted(leaves, key=lambda n: n.prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = torch.rand(1000, 30, device=\"cuda:0\")\n",
    "y = torch.rand(1000, device=\"cuda:0\")\n",
    "tree = DecisionTreeRegressorTorch(max_depth=1000000, min_impurity_decrease=1e-5)\n",
    "tree.fit(X, y)\n",
    "leaves = tree.get_leaves_sorted()\n",
    "\n",
    "len(leaves), np.max([n.depth for n in leaves])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ifera\n",
    "import torch\n",
    "from torch._higher_order_ops.foreach_map import foreach_map\n",
    "\n",
    "\n",
    "fm = ifera.FileManager()\n",
    "url = \"file:data/tensor/futures_rollover/30m/GC.pt\"\n",
    "fm.build_subgraph(url, ifera.RuleType.REFRESH)\n",
    "cm = ifera.ConfigManager()\n",
    "base_instrument = cm.get_base_instrument_config(\"GC\",\"30m\")\n",
    "base_tensor = torch.tensor([])\n",
    "contract_instruments = []\n",
    "contract_tensors = []\n",
    "\n",
    "get_params_time = 0\n",
    "get_tensor_time = 0\n",
    "\n",
    "for dep in fm.refresh_graph.successors(url):\n",
    "    # fm.refresh_file(dep)\n",
    "    params = fm.get_node_params(ifera.RuleType.REFRESH, dep)\n",
    "\n",
    "    if params[\"type\"] == \"futures\":\n",
    "        base_tensor = ifera.load_data_tensor(\n",
    "            instrument=base_instrument,\n",
    "            reset=False,\n",
    "            dtype=torch.float32,\n",
    "            device=torch.device(\"cuda:0\"),\n",
    "            strip_date_time=False,\n",
    "        )\n",
    "    else:\n",
    "        contract_instrument = cm.create_derived_base_config(\n",
    "            base_instrument,\n",
    "            contract_code=params[\"contract_code\"],\n",
    "        )\n",
    "        contract_tensor = ifera.load_data_tensor(\n",
    "            instrument=contract_instrument,\n",
    "            reset=False,\n",
    "            dtype=torch.float32,\n",
    "            device=torch.device(\"cuda:0\"),\n",
    "            strip_date_time=False,\n",
    "        )\n",
    "        contract_tensors.append(contract_tensor)\n",
    "        contract_instruments.append(contract_instrument)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "agg_contract_tensor = torch.full(\n",
    "    (len(contract_tensors), base_tensor.shape[0], base_tensor.shape[1], base_tensor.shape[2]),\n",
    "    torch.nan,\n",
    "    dtype=base_tensor.dtype,\n",
    "    device=base_tensor.device,\n",
    ")\n",
    "\n",
    "contract_tensors = sorted(contract_tensors, key=lambda x: (x[-1, 0, 2].to(torch.int64).item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_contract_tensors_to_agg(contract_tensors, base_tensor, agg_contract_tensor):\n",
    "    \"\"\"\n",
    "    Copies data from contract_tensors to agg_contract_tensor based on matching dates.\n",
    "    \n",
    "    Args:\n",
    "        contract_tensors (list): List of tensors, each shape (date_i, time, features)\n",
    "        base_tensor (torch.Tensor): Shape (date_base, time, features)\n",
    "        agg_contract_tensor (torch.Tensor): Shape (n, date_base, time, features)\n",
    "    \"\"\"\n",
    "    # Extract dates from base_tensor, shape (date_base,)\n",
    "    base_dates = base_tensor[:, 0, 2]\n",
    "    \n",
    "    # Compute matching indices for each contract_tensor\n",
    "    b_indices_list = []\n",
    "    for i in range(len(contract_tensors)):\n",
    "        # Extract dates, shape (date_i,)\n",
    "        contract_dates = contract_tensors[i][:, 0, 2]\n",
    "        # Broadcasting to find matches, shape (date_i, date_base)\n",
    "        matches = contract_dates[:, None] == base_dates[None, :]\n",
    "        # Get base indices, shape (date_i,)\n",
    "        b_indices = matches.long().argmax(dim=1)\n",
    "        b_indices_list.append(b_indices)\n",
    "    \n",
    "    # Concatenate all base indices, shape (total_dates,)\n",
    "    all_b_idx = torch.cat(b_indices_list, dim=0)\n",
    "    \n",
    "    # Create tensor of contract indices, shape (total_dates,)\n",
    "    all_i = torch.cat([\n",
    "        torch.full((contract_tensors[i].shape[0],), i, dtype=torch.long)\n",
    "        for i in range(len(contract_tensors))\n",
    "    ], dim=0)\n",
    "    \n",
    "    # Concatenate all contract data, shape (total_dates, time, features)\n",
    "    all_data = torch.cat(contract_tensors, dim=0)\n",
    "    \n",
    "    # Assign data to agg_contract_tensor in one operation\n",
    "    agg_contract_tensor[all_i, all_b_idx, :, :] = all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_contract_tensors_to_agg(contract_tensors, base_tensor, agg_contract_tensor)\n",
    "agg_contract_tensor.shape, contract_tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "agg_index = (agg_contract_tensor[0, :, 0, 2] == contract_tensors[0][19, 0, 2]).to(torch.int64).argmax()\n",
    "\n",
    "(agg_contract_tensor[:, :, 0, 2].count_nonzero(0) == 0).to(torch.int64).argmax()\n",
    "\n",
    "orddates = base_tensor[:, 0, 2].to(torch.int64).to(\"cpu\").numpy()\n",
    "dates = np.array([dt.date.fromordinal(int(x)) for x in orddates])\n",
    "weekends = np.array([dt.date.weekday(x) for x in dates]) >= 5\n",
    "weekends.sum()\n",
    "\n",
    "base_tensor[623, :, :]\n",
    "\n",
    "#TODO: Remove 0 volume dates (weekends)\n",
    "\n",
    "\n",
    "\n",
    "# Get the indices in dim=0 where agg_contract_tensor[:, 0, 0, 2] is not 0\n",
    "pos = agg_contract_tensor[:, 0, 0, 2].nonzero(as_tuple=True)[0]\n",
    "\n",
    "print(pos)\n",
    "\n",
    "print(np.array2string(base_tensor[0, 0, 4:].cpu().numpy(),\n",
    "    formatter={'float_kind': lambda x: f\"{x:.2f}\"}))\n",
    "    \n",
    "print(np.array2string(agg_contract_tensor[pos, 0, 0, 4:].cpu().numpy(),\n",
    "    formatter={'float_kind': lambda x: f\"{x:.2f}\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "base_features = base_tensor[:, :, 4:8].clone()\n",
    "agg_features = agg_contract_tensor[:, :, :, 4:8]\n",
    "\n",
    "base_features = torch.where(base_tensor[:, :, 8].unsqueeze(-1) == 0, torch.nan, base_features)\n",
    "\n",
    "# matches = torch.all(\n",
    "#         agg_features == base_features[None, :, :, :],\n",
    "#         dim=-1\n",
    "#     )\n",
    "\n",
    "diff_features = agg_features - base_features[None, :, :, :]\n",
    "mse_features = (diff_features ** 2).mean(dim=-1)\n",
    "\n",
    "result = torch.full(\n",
    "    (base_tensor.shape[0], base_tensor.shape[1]),\n",
    "    float('nan'),\n",
    "    dtype=torch.float,\n",
    "    device=base_tensor.device\n",
    ")\n",
    "\n",
    "any_match = mse_features.isnan().logical_not().any(dim=0)\n",
    "first_match_idx = mse_features.nan_to_num(torch.inf).argmin(dim=0)\n",
    "result[any_match] = first_match_idx[any_match].float()\n",
    "\n",
    "\n",
    "d = 3737\n",
    "t = 45\n",
    "\n",
    "print(result[d])\n",
    "# print(np.array2string(result.nan_to_num(-1).max(dim=1)[0][0:200].to(torch.int64).cpu().numpy(), \n",
    "#       formatter={'float_kind': lambda x: f\"{x:.2f}\"}))\n",
    "print(mse_features[first_match_idx[d, t], d, t])\n",
    "print(mse_features[:, d, t].isnan().logical_not().sum())\n",
    "print(base_features[d, t], base_tensor[d, t, 8])\n",
    "print(agg_contract_tensor[first_match_idx[d, t]-2, d, t, 4:8])\n",
    "print(agg_contract_tensor[first_match_idx[d, t]-1, d, t, 4:8])\n",
    "print(agg_contract_tensor[first_match_idx[d, t], d, t, 4:8])\n",
    "print(agg_contract_tensor[first_match_idx[d, t]+1, d, t, 4:8])\n",
    "print(agg_contract_tensor[first_match_idx[d, t]+2, d, t, 4:9])\n",
    "# print(dt.date.fromordinal(base_tensor[d, t, 2].to(torch.int64).item()))\n",
    "print(dt.date.fromordinal(contract_tensors[first_match_idx[d, t]][-1, t, 2].to(torch.int64).item()))\n",
    "# print(dt.datetime(hour=18) + dt.timedelta(minutes=30 * t))\n",
    "trade_dt = dt.datetime.fromordinal(base_tensor[d, 0, 0].to(torch.int64).item()) + dt.timedelta(minutes=30 * t + 18*60)\n",
    "print(trade_dt.strftime(\"%Y-%m-%d (%A) %H:%M:%S\"))\n",
    "\n",
    "# range_max = result.shape[0]\n",
    "range_max = 3737\n",
    "unque_counts = [result[i].unique().isnan().logical_not().sum().cpu().item() for i in range(range_max)]\n",
    "\n",
    "# Print the largest index of unique counts where the count is greater than 1\n",
    "largest_index = max((i for i, count in enumerate(unque_counts) if count > 1), default=None)\n",
    "print(f\"Largest index with more than 1 unique count: {largest_index}, Count: {unque_counts[largest_index] if largest_index is not None else 'N/A'}\")\n",
    "print(unque_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_features = agg_features - base_features[None, :, :, :]\n",
    "mse_features = (diff_features ** 2).mean(dim=-1)\n",
    "\n",
    "mse_features[:, 0,0]\n",
    "\n",
    "print(np.array2string(mse_features[:, 0,0].cpu().numpy(),\n",
    "    formatter={'float_kind': lambda x: f\"{x:.2f}\"}))\n",
    "\n",
    "# diff_features[100, 0, 0], agg_features[100, 0, 0], base_features[0, 0]\n",
    "\n",
    "mse_features.nan_to_num(torch.inf).argmin(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime as dt\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser as dtparse\n",
    "\n",
    "\n",
    "def _extract_date(label: str, soup: BeautifulSoup) -> Optional[dt.date]:\n",
    "    \"\"\"\n",
    "    Find the first element whose text *contains* ``label`` and grab the text in\n",
    "    its next sibling node (works for <td>, <th>, <dt>, <div>, etc.).\n",
    "    Falls back to a regex search through the whole document.\n",
    "    \"\"\"\n",
    "    # 1️⃣ DOM-based search\n",
    "    node = soup.find(string=lambda s: s and label in s)\n",
    "    if node:\n",
    "        sib = node.parent and node.parent.find_next_sibling()\n",
    "        if sib and sib.get_text(strip=True):\n",
    "            return _clean_date(sib.get_text())\n",
    "\n",
    "    # 2️⃣ Regex fall-back (robust to layout changes)\n",
    "    m = re.search(rf\"{re.escape(label)}\\s*([0-9]{{1,2}}/[0-9]{{1,2}}/[0-9]{{2,4}})\",\n",
    "                  soup.get_text(\" \", strip=True), flags=re.I)\n",
    "    return _clean_date(m.group(1)) if m else None\n",
    "\n",
    "\n",
    "def _clean_date(text: str) -> Optional[dt.date]:\n",
    "    if not text:\n",
    "        return None\n",
    "    # Drop trailing notes such as \"(expired)\" or \"(est)\".\n",
    "    date_str = text.split()[0]\n",
    "    return dtparse.parse(date_str, dayfirst=False).date()\n",
    "\n",
    "\n",
    "def contract_notice_and_expiry(symbol: str) -> Tuple[Optional[dt.date], Optional[dt.date]]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    symbol : str\n",
    "        Futures contract symbol as it appears in the Barchart URL\n",
    "        (e.g. ``\"CLF11\"`` for Crude Oil WTI Jan '11).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    first_notice : datetime.date or None\n",
    "    expiration    : datetime.date or None\n",
    "    \"\"\"\n",
    "    url = f\"https://www.barchart.com/futures/quotes/{symbol}\"\n",
    "    resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    first_notice = _extract_date(\"First Notice Date\", soup)\n",
    "    expiration   = _extract_date(\"Expiration Date\", soup)\n",
    "    return first_notice, expiration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_notice_and_expiry(\"GCF23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ifera\n",
    "import torch\n",
    "import datetime as dt\n",
    "\n",
    "fm = ifera.FileManager()\n",
    "url = \"file:data/tensor/futures_individual/30m/CL-G15.pt\"\n",
    "fm.refresh_file(url)\n",
    "\n",
    "cm = ifera.ConfigManager()\n",
    "base_instrument = cm.get_base_instrument_config(\"CL\", \"30m\")\n",
    "instrument = cm.create_derived_base_config(base_instrument, contract_code=\"G15\")\n",
    "t = ifera.load_data_tensor(\n",
    "    instrument=instrument,\n",
    "    dtype=torch.float32,\n",
    "    device=torch.device(\"cuda:0\"),\n",
    "    strip_date_time=False,\n",
    ")\n",
    "\n",
    "# dt.date.fromordinal(int(t[0, 0, 2].item())), dt.date.fromordinal(int(t[-19, 0, 2].item())), t.shape\n",
    "\n",
    "t[-16, :, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ifera\n",
    "\n",
    "cf = ifera.ConfigManager()\n",
    "instrument = cf.get_base_instrument_config(\"ES\", \"30m\")\n",
    "contract_instrument = cf.create_derived_base_config(instrument, contract_code=\"M21\")\n",
    "\n",
    "ifera.calculate_expiration(contract_instrument.contract_code, ifera.ExpirationRule(contract_instrument.last_trading_day_rule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot = df[\"offset_time\"]\n",
    "\n",
    "# Count na values in the 'offset_time' column\n",
    "df = pd.DataFrame(columns=[\n",
    "            'open', 'high', 'low', 'close', 'volume',\n",
    "            'date', 'time', 'offset_time', 'trade_date'\n",
    "        ]).astype({\n",
    "            'open': 'float32',\n",
    "            'high': 'float32', \n",
    "            'low': 'float32',\n",
    "            'close': 'float32',\n",
    "            'volume': 'int32',\n",
    "            'date': 'datetime64[ns]',\n",
    "            'time': 'timedelta64[ns]',\n",
    "            'offset_time': 'timedelta64[ns]',\n",
    "            'trade_date': 'datetime64[ns]'\n",
    "        })\n",
    "        \n",
    "# Add a row with some data\n",
    "df.loc[0] = [\n",
    "    3.5, 4.0, 3.0, 3.8, 1000,\n",
    "    pd.Timestamp('2024-01-01'), pd.Timedelta('09:30:00'), pd.Timedelta('09:30:00'), pd.Timestamp('2024-01-01')\n",
    "]\n",
    "\n",
    "offset_seconds = -6 * 3600  # Convert hours to seconds\n",
    "SECONDS_IN_DAY = 86400\n",
    "\n",
    "\n",
    "df[\"offset_time\"].map(lambda x: x.total_seconds()).add(offset_seconds).mod(SECONDS_IN_DAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ifera\n",
    "\n",
    "for obj in ifera.list_s3_objects('meta/futures/dates'):\n",
    "    ifera.delete_s3_file(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import time\n",
    "\n",
    "def task(n):\n",
    "    time.sleep(1)\n",
    "    return n * n\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    futures = [executor.submit(task, i) for i in range(5)]\n",
    "    for future in futures:\n",
    "        print(f\"Result: {future.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ifera import ThreadSafeCache\n",
    "\n",
    "@ThreadSafeCache(maxsize=100)\n",
    "def example_function(x: int) -> int:\n",
    "    return x * x\n",
    "\n",
    "\n",
    "example_function.cache_clear()\n",
    "print(example_function(x=5))\n",
    "print(example_function(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_function(x=5))\n",
    "print(example_function(5))\n",
    "example_function.cache_clear()  # Clears the cache\n",
    "print(example_function(5))  # Recomputes: 25\n",
    "print(example_function(5))  # Recomputes: 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ifera\n",
    "\n",
    "cm = ifera.ConfigManager()\n",
    "fm = ifera.FileManager()\n",
    "\n",
    "broker = cm.get_broker_config(\"IBKR\")\n",
    "symbols = broker.instruments.keys()\n",
    "\n",
    "for symbol in symbols:\n",
    "    print(f\"Refreshing metadata for {symbol}...\")\n",
    "    iconfig = cm.get_base_instrument_config(symbol, \"30m\")\n",
    "    url = f\"file:data/meta/futures/dates/{symbol}.yml\"\n",
    "\n",
    "    fm.refresh_file(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ifera\n",
    "\n",
    "for s3_key in ifera.list_s3_objects(\"\"):\n",
    "    if s3_key.endswith(\".pt\"):\n",
    "        print(f\"Deleting S3 file: {s3_key}\")\n",
    "        ifera.delete_s3_file(s3_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.torch_version\n",
    "import torch.version\n",
    "import gzip\n",
    "\n",
    "t = torch.rand(100, 100, 10, dtype=torch.float32, device=\"cuda:0\")\n",
    "\n",
    "with gzip.open(\"test.pt.gz\", \"wb\") as f:\n",
    "    torch.save(t, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ifera\n",
    "import torch\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "fm = ifera.FileManager()\n",
    "symbol = \"AD\"\n",
    "\n",
    "url = f\"file:data/meta/futures/rollover/{symbol}.yml\"\n",
    "fm.build_subgraph(url, ifera.RuleType.REFRESH)\n",
    "cm = ifera.ConfigManager()\n",
    "base_instrument = cm.get_base_instrument_config(symbol,\"30m\")\n",
    "contract_instruments = []\n",
    "contract_tensors = []\n",
    "\n",
    "get_params_time = 0\n",
    "get_tensor_time = 0\n",
    "\n",
    "# Load data/meta/futures/dates/AD.yml\n",
    "# fm.refresh_file(f\"file:data/meta/futures/dates/{symbol}.yml\")\n",
    "with open(f\"data/meta/futures/dates/{symbol}.yml\", \"r\") as f:\n",
    "    dates = yaml.safe_load(f)\n",
    "\n",
    "for dep in fm.refresh_graph.successors(url):\n",
    "    if not dep.startswith(\"file:data/tensor/futures_individual/30m/\"):\n",
    "        continue\n",
    "    \n",
    "    # fm.refresh_file(dep)\n",
    "    params = fm.get_node_params(ifera.RuleType.REFRESH, dep)\n",
    "\n",
    "    contract_instrument = cm.create_derived_base_config(\n",
    "        base_instrument,\n",
    "        contract_code=params[\"contract_code\"],\n",
    "    )\n",
    "    # contract_instrument.rollover_vol_alpha = 1.0\n",
    "    if dates[contract_instrument.contract_code][\"first_notice_date\"] is not None:\n",
    "        contract_instrument.first_notice_date = dt.date.fromisoformat(dates[contract_instrument.contract_code][\"first_notice_date\"])\n",
    "\n",
    "    if dates[contract_instrument.contract_code][\"expiration_date\"] is not None:\n",
    "        contract_instrument.expiration_date = dt.date.fromisoformat(dates[contract_instrument.contract_code][\"expiration_date\"])\n",
    "\n",
    "    contract_tensor = ifera.load_data_tensor(\n",
    "        instrument=contract_instrument,\n",
    "        dtype=torch.float64,\n",
    "        device=torch.device(\"cuda:0\"),\n",
    "        strip_date_time=False,\n",
    "    )\n",
    "    contract_tensors.append(contract_tensor)\n",
    "    contract_instruments.append(contract_instrument)\n",
    "\n",
    "# Sort both contract_instruments and contract_tensors by expiration date\n",
    "sorted_indices = sorted(range(len(contract_instruments)), key=lambda k: contract_instruments[k].expiration_date)\n",
    "contract_instruments_sorted = [contract_instruments[i] for i in sorted_indices]\n",
    "contract_tensors_sorted = [contract_tensors[i] for i in sorted_indices]\n",
    "\n",
    "ratios, active_idx, all_dates_ord = ifera.calculate_rollover(contract_instruments_sorted, contract_tensors_sorted)\n",
    "# ratios = ratios.nan_to_num(1.0)\n",
    "\n",
    "# # Calculate the cumulative product of the ratios backwards\n",
    "# ratios = torch.flip(ratios, dims=[0])\n",
    "# ratios = torch.cumprod(ratios, dim=0)\n",
    "# ratios = torch.flip(ratios, dims=[0])\n",
    "print(ratios.shape, ratios.dtype)\n",
    "\n",
    "# print(np.array2string(ratios.cpu().numpy(),\n",
    "#     formatter={'float_kind': lambda x: f\"{x:.4f}\"}))\n",
    "# print(np.array2string(active_idx.cpu().numpy(),\n",
    "#     formatter={'int_kind': lambda x: f\"{x}\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios[~ratios.isnan()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices where rollover occurs (non-NaN values in ratios), plus the first index\n",
    "rollover_indices = torch.nonzero(~ratios.isnan(), as_tuple=True)[0]\n",
    "rollover_indices = torch.cat((torch.tensor([0], device=rollover_indices.device, dtype=rollover_indices.dtype), rollover_indices), dim=0)\n",
    "\n",
    "# Calculate the cumulative product of the ratios backwards\n",
    "ratios_tmp = torch.cat((ratios.nan_to_num(1.0), torch.ones(1, device=ratios.device, dtype=ratios.dtype)), dim=0)\n",
    "ratios_tmp = torch.flip(ratios_tmp, dims=[0])\n",
    "ratios_cump = torch.cumprod(ratios_tmp, dim=0)\n",
    "ratios_cump = torch.flip(ratios_cump, dims=[0]).to(torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollover_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "dt.date.fromordinal(contract_tensors_sorted[190][-1, 0, 2].cpu().to(torch.int64).item())\n",
    "\n",
    "ratios_cump[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = active_idx[rollover_indices].cpu().numpy()\n",
    "\n",
    "# Get the contract code from contract_instruments_sorted based on idx\n",
    "contract_codes = [contract_instruments_sorted[i].contract_code for i in pos]\n",
    "print(contract_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollover_spec = []\n",
    "\n",
    "for active, pos, ratio in zip(active_idx[rollover_indices].cpu().numpy(), rollover_indices.cpu().numpy(), ratios_cump[rollover_indices+1].cpu().numpy()):\n",
    "    contract_code = contract_instruments_sorted[active].contract_code\n",
    "    rollover_spec.append({\n",
    "        \"start_date\": dt.date.fromordinal(all_dates_ord[pos]),\n",
    "        \"contract_code\": contract_code,\n",
    "        \"multiplier\": ratio.item()\n",
    "    })\n",
    "    \n",
    "import yaml\n",
    "# with open(\"data/meta/futures/rollover/GC.yml\", \"w\") as f:\n",
    "#     yaml.dump(rollover_spec, f, default_flow_style=False, sort_keys=False)\n",
    "print(yaml.dump(rollover_spec, default_flow_style=False, sort_keys=False))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(dt.timedelta(seconds= base_instrument.rollover_offset if pos > 0 else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[1:3, :, 8]\n",
    "\n",
    "print(np.array2string(t[1:3, :, 8].cpu().numpy(),\n",
    "    formatter={'float_kind': lambda x: f\"{x:.4f}\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_instrument = cm.create_derived_base_config(\n",
    "        base_instrument,\n",
    "        contract_code=\"Q15\",\n",
    "    )\n",
    "contract_tensor = ifera.load_data_tensor(\n",
    "        instrument=contract_instrument,\n",
    "        dtype=torch.float64,\n",
    "        device=torch.device(\"cuda:0\"),\n",
    "        strip_date_time=False,\n",
    "    )\n",
    "\n",
    "print(np.array2string(contract_tensor[2, :, 8].cpu().numpy(),\n",
    "    formatter={'float_kind': lambda x: f\"{x:.4f}\"}))\n",
    "\n",
    "contract_tensor[2, :, 8].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ifera\n",
    "import torch\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "contract_code = \"J15\"\n",
    "\n",
    "fm = ifera.FileManager()\n",
    "cm = ifera.ConfigManager()\n",
    "base_instrument = cm.get_base_instrument_config(\"GC\",\"30m\")\n",
    "contract_instrument = cm.create_derived_base_config(base_instrument, contract_code=contract_code)\n",
    "fm.refresh_file(f\"file:data/raw/futures_individual/30m/GC-{contract_code}.zip\")\n",
    "df30 = ifera.load_data(raw=True, instrument=contract_instrument)\n",
    "\n",
    "# base_instrument_1 = cm.get_base_instrument_config(\"GC\",\"1m\") \n",
    "# contract_instrument_1 = cm.create_derived_base_config(base_instrument_1, contract_code=contract_code)\n",
    "# fm.refresh_file(f\"file:data/raw/futures_individual/1m/GC-{contract_code}.zip\")\n",
    "# df1 = ifera.load_data(raw=True, instrument=contract_instrument_1)\n",
    "\n",
    "df30[df30.index >= pd.Timestamp(\"2015-02-06 08:30:00\")].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "print(dt.date.fromordinal(735635))\n",
    "df30[df30.index >= pd.Timestamp(\"2015-02-06 08:30:00\")].head()\n",
    "\n",
    "# print(df1[df1.index >= pd.Timestamp(\"2015-02-09 08:30:00\")].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holidays.financial import ny_stock_exchange\n",
    "\n",
    "US_HOLIDAYS = ny_stock_exchange.NewYorkStockExchange()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
